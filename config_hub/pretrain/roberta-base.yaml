# RoBERTa-base pretraining configuration
# Based on the original RoBERTa paper settings

model:
  name: "roberta-base"
  n_layer: 12
  n_embd: 768
  vocab_size: 50265
  block_size: 512
  bias: true

data:
  data_path: "data/wikitext"  # Path to your streaming dataset
  block_size: 512
  micro_batch_size: 8  # Batch size per GPU
  val_batch_size: 8
  num_workers: 4
  seed: 42

train:
  devices: "auto"  # Use all available GPUs
  precision: "16-mixed"  # Use mixed precision training
  epochs: 3
  log_interval: 100
  gradient_clip_val: 1.0
  gradient_accumulation_steps: 1  # Adjust based on your needs

eval:
  # Number of optimizer steps between evaluation calls
  interval: 100

  # Number of iterations for evaluation
  max_iters: 100

  # Whether to evaluate on the validation set at the beginning of the training
  initial_validation: true

  # Whether to evaluate on the validation set at the end the training
  final_validation: true

# Optimizer-related arguments
optimizer:
  class_path: "torch.optim.AdamW"
  init_args:
    lr: 1e-4
    betas: [0.9, 0.999]
    eps: 1e-8
    weight_decay: 0.01

# Learning rate scheduler settings
lr_scheduler:
  class_path: "torch.optim.lr_scheduler.LambdaLR"
  init_args:
    warmup_steps: 1000

output_dir: "out/roberta_wikitext" 